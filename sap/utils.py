from common.html_parser import HTMLParser
from common.json_writer import JSONWriter
from sap.row_parser import RowParser
from common.utils import dump_as_json
from datetime import datetime


def get_publish_date_string(parser, date_div_class):
    if date_div_class == "main-content":
        date_para = parser.get_tag_by_id(date_div_class)
    else :
        date_para = parser.get_tag_by_class(date_div_class)
    p_soup = date_para.find_all("p")
    p_text = p_soup[1].text
    p_text = p_text.split(",", 2)[0]    
    return p_text

def get_outer_div(parser, div_class):
    div = parser.get_tag_by_class(div_class)
    return div 

def get_title(parser, title_soup):
    if title_soup is None:
        title = parser.get_head_title()
        return title

    title = title_soup.a.text.strip()
    title = title.replace("\u00e2\u0080\u0093", "").replace("\u2013", "")
    return title

def get_security_alerts_json(vendor, urls):
    cves = list()
    failed_cvs_count = 0
    i = 0
    # print(dates)
    for url in urls:
        is_old_layout_format = False
        div_class = "table-wrap" 
        date_div_class = "main-content"

        parser = HTMLParser(url)
        title_soup = parser.get_tag_by_id("title-text")

        if title_soup is None:
            # old layout
            div_class = "scn-scrollable-area"
            date_div_class = "dm-contentDetail__body-content" 

        title = get_title(parser, title_soup)
        # print(title)

        outer_div = get_outer_div(parser, div_class)
        alerts = parser.get_table_body_rows(outer_div)
        # alerts = [alerts[-9]]
        # alerts = [alerts[0],alerts[1],alerts[2],alerts[3], alerts[4], alerts[5], alerts[6]]
        # print(alerts)
        publisging_date = get_publish_date_string(parser, date_div_class)

        row_parser = RowParser(publisging_date)

        writer = JSONWriter(alerts, row_parser)
        (cves_per_url, failed_cvs_count_per_url) = writer.parse_alerts(
            vendor, url, None, title)
        cves += cves_per_url
        failed_cvs_count += failed_cvs_count_per_url
    return (dump_as_json(vendor,cves), len(cves),failed_cvs_count)

def get_publish_date(date_string, allow_print= False):
    if allow_print:
        print(date_string)
    return datetime.strptime(date_string, "%B %Y") 

def should_parse_bulletin(text, from_date, to_date):
    date = get_publish_date(text)
    if from_date is None and to_date is None:
        return True
    elif from_date is None and date <= to_date:
        return True
    elif to_date is None and date >= from_date :
        return True
    elif date >= from_date and date <= to_date :
        return True
    return False

